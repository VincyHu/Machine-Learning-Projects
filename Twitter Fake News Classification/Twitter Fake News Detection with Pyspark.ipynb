{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter fake news detection model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will develop fake news detection model using the pre-labeling fake news dataset provided by Polifact, a third-party non-profit organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the dataset provided, we have two features: **news url** and **news title**, we will analyze the text from those two features to try to predict whether a twitter news is fake or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from itertools import chain\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType\n",
    "from collections import Counter\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import array_contains\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier,LogisticRegression\n",
    "import tweepy\n",
    "import time\n",
    "from kafka import KafkaConsumer,KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "sc=SparkContext(appName='PySparkShell')\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------------+--------------------+--------------------+-----+\n",
      "|             id|            news_url|               title|label|\n",
      "+---------------+--------------------+--------------------+-----+\n",
      "|politifact15014|speedtalk.com/for...|BREAKING: First N...|    1|\n",
      "|politifact15156|politics2020.info...|Court Orders Obam...|    1|\n",
      "|politifact14745|www.nscdscamps.or...|UPDATE: Second Ro...|    1|\n",
      "|politifact14355|https://howafrica...|Oscar Pistorius A...|    1|\n",
      "|politifact15371|http://washington...|Trump Votes For D...|    1|\n",
      "|politifact14404|gloria.tv/video/y...|Putin says: ?�?Po...|    1|\n",
      "|politifact13919|http://blogs.tren...|New York Man Want...|    1|\n",
      "|politifact14795|https://web.archi...|Saudi Arabia to B...|    1|\n",
      "|politifact14328|https://web.archi...|Malia Obama Fired...|    1|\n",
      "|politifact13775|http://beforeitsn...|Target to Discont...|    1|\n",
      "|politifact14678|http://yournewswi...|Youngest World Le...|    1|\n",
      "|politifact14394|https://web.archi...|BREAKING: Hillary...|    1|\n",
      "|politifact14376|http://dailyfeed....|Barack Obama Twee...|    1|\n",
      "|politifact14233|jis.gov.jm/presid...|President Trump U...|    1|\n",
      "|politifact14890|me.me/i/actress-s...|Actress Sandra Bu...|    1|\n",
      "|politifact14356|http://therightis...|Gretchen Carlson:...|    1|\n",
      "|politifact14776|http://yournewswi...|Pope Francis At M...|    1|\n",
      "|politifact15355|https://web.archi...|Black Men Arreste...|    1|\n",
      "|politifact14664|http://www.breitb...|ICE Detainer Issu...|    1|\n",
      "|politifact15178|https://www.polit...|Former presidents...|    1|\n",
      "+---------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read file \n",
    "# to read parquet file\n",
    "data=spark.read.csv('combined_polifact.csv',inferSchema=True, header = True)\n",
    "print(type(data))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect for null value \n",
    "data = data.filter(data.news_url. isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column name for original label\n",
    "data = data.withColumnRenamed(\"label\", \"fake\") # change the original dataset label name to others,'label' will be used in later stage\n",
    "\n",
    "#clean the news_url\n",
    "new_data = data.withColumn(\"url_1\",regexp_replace(col(\"news_url\"), \"(https|http):\\/\\/(www|web)*\\.*\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+----+--------------------+\n",
      "|             id|            news_url|               title|fake|               url_1|\n",
      "+---------------+--------------------+--------------------+----+--------------------+\n",
      "|politifact15014|speedtalk.com/for...|BREAKING: First N...|   1|speedtalk.com/for...|\n",
      "|politifact15156|politics2020.info...|Court Orders Obam...|   1|politics2020.info...|\n",
      "|politifact14745|www.nscdscamps.or...|UPDATE: Second Ro...|   1|www.nscdscamps.or...|\n",
      "|politifact14355|https://howafrica...|Oscar Pistorius A...|   1|howafrica.com/osc...|\n",
      "|politifact15371|http://washington...|Trump Votes For D...|   1|washingtonsources...|\n",
      "|politifact14404|gloria.tv/video/y...|Putin says: ?�?Po...|   1|gloria.tv/video/y...|\n",
      "|politifact13919|http://blogs.tren...|New York Man Want...|   1|blogs.trendolizer...|\n",
      "|politifact14795|https://web.archi...|Saudi Arabia to B...|   1|archive.org/web/2...|\n",
      "|politifact14328|https://web.archi...|Malia Obama Fired...|   1|archive.org/web/2...|\n",
      "|politifact13775|http://beforeitsn...|Target to Discont...|   1|beforeitsnews.com...|\n",
      "|politifact14678|http://yournewswi...|Youngest World Le...|   1|yournewswire.com/...|\n",
      "|politifact14394|https://web.archi...|BREAKING: Hillary...|   1|archive.org/web/2...|\n",
      "|politifact14376|http://dailyfeed....|Barack Obama Twee...|   1|dailyfeed.news/ba...|\n",
      "|politifact14233|jis.gov.jm/presid...|President Trump U...|   1|jis.gov.jm/presid...|\n",
      "|politifact14890|me.me/i/actress-s...|Actress Sandra Bu...|   1|me.me/i/actress-s...|\n",
      "|politifact14356|http://therightis...|Gretchen Carlson:...|   1|therightists.com/...|\n",
      "|politifact14776|http://yournewswi...|Pope Francis At M...|   1|yournewswire.com/...|\n",
      "|politifact15355|https://web.archi...|Black Men Arreste...|   1|archive.org/web/2...|\n",
      "|politifact14664|http://www.breitb...|ICE Detainer Issu...|   1|breitbart.com/cal...|\n",
      "|politifact15178|https://www.polit...|Former presidents...|   1|politico.com/stor...|\n",
      "+---------------+--------------------+--------------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWordsRemover.loadDefaultStopWords('english')\n",
    "\n",
    "# #### For title #####\n",
    "# 0. Extract tokens from title\n",
    "tokenizer_title= RegexTokenizer(inputCol= 'url_1', outputCol= 'url_words',pattern= '\\\\W', toLowercase= True)\n",
    "\n",
    "# 1. Remove stop words from title\n",
    "title_sw_remover= StopWordsRemover(inputCol= 'url_words', outputCol= 'url_stop_removed')\n",
    "\n",
    "# 2. Compute Term frequency from title\n",
    "title_count_vectorizer= CountVectorizer(inputCol= 'url_stop_removed', outputCol= 'url_tf')\n",
    "\n",
    "# 3. Compute Term frequency-inverse document frequency from title\n",
    "title_tfidf= IDF(inputCol= 'url_tf', outputCol= 'tf_idf_url')\n",
    "\n",
    "#### For title text ####\n",
    "# 4. Extract tokens from text\n",
    "text_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'text_words',\n",
    "                                pattern= '\\\\W', toLowercase= True)\n",
    "\n",
    "# 5. Remove stop words from text\n",
    "add_stopwords_title = [\"http\",\"https\",\"amp\",\"rt\",\"www\"] \n",
    "text_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed').setStopWords(add_stopwords_title)\n",
    "\n",
    "# 6. Compute Term frequency from text (BagofWords Count)\n",
    "text_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n",
    "\n",
    "# 7. Compute Term frequency-inverse document frequency text\n",
    "text_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n",
    "\n",
    "# 8. StringIndexer subject\n",
    "labelIndexer= StringIndexer(inputCol= 'fake', outputCol= 'label')\n",
    "\n",
    "# 9. VectorAssembler\n",
    "vec_assembler= VectorAssembler(inputCols=['tf_idf_url', 'tf_idf_text'], outputCol= 'features')\n",
    "\n",
    "# 10 logistic regression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Build the pipeline to deal with document\n",
    "LRpipeline = Pipeline(stages=[tokenizer_title,\n",
    "                            title_sw_remover,\n",
    "                            title_count_vectorizer,\n",
    "                            title_tfidf,\n",
    "                            text_tokenizer, \n",
    "                            text_sw_remover, \n",
    "                            text_count_vectorizer,\n",
    "                            text_tfidf,\n",
    "                            vec_assembler,\n",
    "                            labelIndexer,\n",
    "                            lr])\n",
    "\n",
    "# set seed for reproducibility\n",
    "(trainingData, testData) = new_data.randomSplit([0.7, 0.3], seed = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = LRpipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and test on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineFit.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435317154291192"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluating classification model\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "accuracy= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'prediction', metricName= 'accuracy')\n",
    "f1= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'prediction', metricName= 'f1')\n",
    "areaUnderROC= BinaryClassificationEvaluator(labelCol= 'label', metricName= 'areaUnderROC')\n",
    "# Since it's both important to avoid misinformation spread and to not incorrectly flag those correct information, we here focus on the f1 and AUC most.\n",
    "\n",
    "def classification_evaluator(data_result):\n",
    "    data_result.crosstab(col1= 'prediction', col2= 'label').show()\n",
    "    print('f1:' ,f1.evaluate(data_result))\n",
    "    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+---+\n",
      "|prediction_label|0.0|1.0|\n",
      "+----------------+---+---+\n",
      "|             1.0|  1| 88|\n",
      "|             0.0|179| 46|\n",
      "+----------------+---+---+\n",
      "\n",
      "f1: 0.8435317154291192\n",
      "areaUnderROC: 0.9737562189054717\n"
     ]
    }
   ],
   "source": [
    "classification_evaluator(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF model with Logistic Regressor can give us AUC 0.97, which is a pretty good result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf= RandomForestClassifier(featuresCol= 'features', labelCol= 'label', predictionCol= 'prediction', maxDepth= 10, numTrees= 20)\n",
    "\n",
    "# Build the pipeline to deal with document\n",
    "RFpipeline = Pipeline(stages=[tokenizer_title,\n",
    "                            title_sw_remover,\n",
    "                            title_count_vectorizer,\n",
    "                            title_tfidf,\n",
    "                            text_tokenizer, \n",
    "                            text_sw_remover, \n",
    "                            text_count_vectorizer,\n",
    "                            text_tfidf,\n",
    "                            vec_assembler,\n",
    "                            labelIndexer,\n",
    "                            rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "RFpipelineFit = RFpipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and test on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFpredictions = RFpipelineFit.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435317154291192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluating classification model\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "accuracy= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'prediction', metricName= 'accuracy')\n",
    "f1= MulticlassClassificationEvaluator(labelCol= 'label', predictionCol= 'prediction', metricName= 'f1')\n",
    "areaUnderROC= BinaryClassificationEvaluator(labelCol= 'label', metricName= 'areaUnderROC')\n",
    "# Since it's both important to avoid misinformation spread and to not incorrectly flag those correct information, we here focus on the f1 and AUC most.\n",
    "\n",
    "def classification_evaluator(data_result):\n",
    "    data_result.crosstab(col1= 'prediction', col2= 'label').show()\n",
    "    print('f1:' ,f1.evaluate(data_result))\n",
    "    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+---+\n",
      "|prediction_label|0.0|1.0|\n",
      "+----------------+---+---+\n",
      "|             1.0| 10| 70|\n",
      "|             0.0|170| 64|\n",
      "+----------------+---+---+\n",
      "\n",
      "f1: 0.7499670011413724\n",
      "areaUnderROC: 0.9150704809286897\n"
     ]
    }
   ],
   "source": [
    "classification_evaluator(RFpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier provides a inperior result than Logistic Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Twitter Fake news",
  "notebookId": 3950038363450814
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
